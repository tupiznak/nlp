# NLP
Ремарка: т.к. все вычисления производились на мало-настроенной для разработки Windows, обобщение выглядит в виде вынесенных блоков в папке src. Все же запуски производились в ноутбуках - в данном случае было в разы удобнее.

### [Решение 0](https://github.com/tupiznak/nlp/blob/main/manual.ipynb)
Первоначально хотелось проверить, что задача вообще решается (до этого подобное не решал). Поэтому взял классическую модель Helsinki-NLP/opus-mt-en-ru, сделал процесс обучения в привычном lightning и получил 30 BLEU. Своего рода бейзлайн.

### BERT2BERT
#### [bert2bert old](https://github.com/tupiznak/nlp/blob/main/bert2bert.ipynb)
После получения адекватной метрики решил начать с берта. Довольно классическим решением оказался bert2bert. Однако проблема была в том, что 2 года назад библиотека transformers была изменена без обратной совместимости. Из-за чего, если воспроизводить по старым мануалам (а bert2bert уже больше 2ух лет), то ничего не работает... Поэтому я изучал код библиотеки, нашел PR, который порушил совместимость и минимально восстановил прежнюю версию. Получился 33 BLEU, что являлось самым большим результатом. Что довольно странно, т.к. далее были пременены улучшения.
#### [bert2bert new](https://github.com/tupiznak/nlp/blob/main/bert2bert_new.ipynb)
Затем последовало долгое изучение кода с целью понять, почему же в новой версии не работает. Оказалось всё как всегда (нет) довольно просто - нужно было убрать decoder_input_ids из датасета, т.к. поле теперь вычисляется в модели само. Чтобы это понять было написано много кода, который, в итоге не стал как-то сохранять, т.к. на конечный результат не влияет. начал формировать папку [src](https://github.com/tupiznak/nlp/tree/main/src) для унификации. BLEU 27.
#### [bert2bert + collator](https://github.com/tupiznak/nlp/blob/main/bert2bert_collator.ipynb)
Разбирался с тем, как преобразовать датасет так, чтобы использовать коллатор (чтобы делать padding не max_length как раньше, а обрезать по батчу). Далее код в ноутбуках унифицировался стал почти одинаковым. BLEU 29.

### BERT2GPT
#### [bert2gpt old](https://github.com/tupiznak/nlp/blob/main/bert2gpt.ipynb)
До предыдущего шага (до коллатора) запускал bert2gpt, но результаты были очень плохими - BLEU 21.
#### [bert2gpt](https://github.com/tupiznak/nlp/blob/main/bert2gpt_collator.ipynb)
После добавления коллатора и детального анализа GPT было определено, что важно очень правильно настроить конфиг. Вероятно так и не настроил правильно, но BLEU 24. Судя по примером ответов (который в конце каждого из ноутбуков), GPT "несёт" - какие-то проблемы с EOS токеном или что-то в этом роде.

### mT5
#### [T5](https://github.com/tupiznak/nlp/blob/main/t5.ipynb)
Изначально взял t5, т.к. она обучена на down-stream задачах в отличие от mT5, но после обучения вдруг обнаружил, что не все русские буквы присутствуют в токенах. При этом из-за того что в compute_metric ф-ии BLEU считалось в сравнении с декодированной label (получалось что-то типа "Как <unk>е<unk>а?"), то и BLEU на валидации был высокий - 33. Но, как только шло сравнение с реальной строкой в compute_metric_test, всё шло не по плану. BLEU 6 - оставил для истории.
#### [mT5](https://github.com/tupiznak/nlp/blob/main/mt5.ipynb)
Здесь сказать особо нечего: просто mT5 single-task. BLEU 28.
#### [mT5 multitask](https://github.com/tupiznak/nlp/blob/main/mt5-multitask.ipynb)
В конце решил попробовать улучшить качество mT5 с помощью первода в обе стороны. Пришлось немного поменять стандартный ноутбук. Обучалось в 2 раза долше, т.к. датасет стал в 2 раза болше (каждый пример встречается как перевод в одну сторону, так и в другую). Проверку на тесте оставил как и в предыдущих ноутбуках. BLEU 30.
  
## Резюме
bert2bert себя показало как самой стабильной моделью. bert2gpt - несло (возможно так и не смог правильно настроить), но была самой быстрой. mT5 - хорошая многофункциональная модель.
  
Несколько красивых графиков без подписей ![image](https://github.com/tupiznak/nlp/assets/17914777/1086887e-28b9-41cc-9832-c159b40ea620)
![image](https://github.com/tupiznak/nlp/assets/17914777/89ccb332-0036-4302-98f0-e70ac22d7ef3)

